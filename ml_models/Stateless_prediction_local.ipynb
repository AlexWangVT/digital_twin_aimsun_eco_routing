{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2cc070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_imaging' from 'PIL' (D:\\program\\Python\\Python37\\Lib\\site-packages\\PIL\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatetime\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "File \u001b[1;32md:\\program\\anaconda3\\envs\\digitaltwin\\lib\\site-packages\\matplotlib\\__init__.py:158\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m parse \u001b[39mas\u001b[39;00m parse_version\n\u001b[0;32m    156\u001b[0m \u001b[39m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    159\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    160\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m \u001b[39mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32md:\\program\\anaconda3\\envs\\digitaltwin\\lib\\site-packages\\matplotlib\\rcsetup.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, cbook\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolors\u001b[39;00m \u001b[39mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_fontconfig_pattern\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_enums\u001b[39;00m \u001b[39mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[1;32md:\\program\\anaconda3\\envs\\digitaltwin\\lib\\site-packages\\matplotlib\\colors.py:51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumbers\u001b[39;00m \u001b[39mimport\u001b[39;00m Number\n\u001b[0;32m     50\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mPngImagePlugin\u001b[39;00m \u001b[39mimport\u001b[39;00m PngInfo\n\u001b[0;32m     54\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n",
      "File \u001b[1;32mD:\\program\\Python\\Python37\\Lib\\site-packages\\PIL\\Image.py:103\u001b[0m\n\u001b[0;32m     94\u001b[0m MAX_IMAGE_PIXELS \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m4\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[39m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[39m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[39m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[39m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _imaging \u001b[39mas\u001b[39;00m core\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m __version__ \u001b[39m!=\u001b[39m \u001b[39mgetattr\u001b[39m(core, \u001b[39m\"\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    106\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    107\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCore version: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mgetattr\u001b[39m(core,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPillow version: \u001b[39m\u001b[39m{\u001b[39;00m__version__\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_imaging' from 'PIL' (D:\\program\\Python\\Python37\\Lib\\site-packages\\PIL\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "print(\"hello\")\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcef335e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_c_internal_utils' from partially initialized module 'matplotlib' (most likely due to a circular import) (D:\\program\\Python\\Python37\\Lib\\site-packages\\matplotlib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatetime\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjoblib\u001b[39;00m\n",
      "File \u001b[1;32mD:\\program\\Python\\Python37\\Lib\\site-packages\\matplotlib\\__init__.py:109\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m parse \u001b[39mas\u001b[39;00m parse_version\n\u001b[0;32m    107\u001b[0m \u001b[39m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, _version, cbook, docstring, rcsetup\n\u001b[0;32m    110\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m MatplotlibDeprecationWarning, sanitize_sequence\n\u001b[0;32m    111\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m mplDeprecation  \u001b[39m# deprecated\u001b[39;00m\n",
      "File \u001b[1;32mD:\\program\\Python\\Python37\\Lib\\site-packages\\matplotlib\\cbook\\__init__.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, _c_internal_utils\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     MatplotlibDeprecationWarning, mplDeprecation)\n\u001b[0;32m     36\u001b[0m \u001b[39m@_api\u001b[39m\u001b[39m.\u001b[39mdeprecated(\u001b[39m\"\u001b[39m\u001b[39m3.4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeprecated\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_c_internal_utils' from partially initialized module 'matplotlib' (most likely due to a circular import) (D:\\program\\Python\\Python37\\Lib\\site-packages\\matplotlib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed, RepeatVector, Lambda\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41760a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable parameters\n",
    "DATA_FILE_PATH = '/Users/dharmik/Downloads/compressed-results-2023-05-05 00_00_00.parquet'\n",
    "MODEL_SAVE_PATH = '/Users/dharmik/Desktop/Model1'\n",
    "ENCODER_LOAD_PATH = '/Users/dharmik/Downloads/mac-RA/encoder.joblib'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "N_STEPS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10149d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset columns\n",
    "REQUIRED_COLUMNS = ['min_osm_id', 'timestamp', 'speed', 'freeFlow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b1c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan_values(df):\n",
    "    # drop rows with NaN values in the dataframe\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df66a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime(timestamp):\n",
    "    try:\n",
    "        return pd.to_datetime(timestamp, format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    except ValueError:\n",
    "        return pd.to_datetime(timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def rec_data_prp(data):\n",
    "    if not set(REQUIRED_COLUMNS).issubset(data.columns):\n",
    "        raise ValueError(f\"Dataset must contain the following columns: {REQUIRED_COLUMNS}\")\n",
    "    data = data.sort_values(by=['min_osm_id', 'timestamp'])\n",
    "    data['timestamp'] = data['timestamp'].apply(convert_datetime)\n",
    "    data['time'] = data['timestamp'].dt.strftime('%H')\n",
    "    data['day'] = data['timestamp'].dt.weekday\n",
    "    data['day'] = data['day'].apply(lambda x: '1' if x <= 4 else '0')  # weekday is 1 and weekend is 0\n",
    "    data['target15'] = data.speed.shift(-3)\n",
    "    data['target15'].fillna(data['freeFlow'], inplace=True)\n",
    "    data['target30'] = data.speed.shift(-6)\n",
    "    data['target30'].fillna(data['freeFlow'], inplace=True)\n",
    "    data['target45'] = data.speed.shift(-9)\n",
    "    data['target45'].fillna(data['freeFlow'], inplace=True)\n",
    "    data['target60'] = data.speed.shift(-12)\n",
    "    data['target60'].fillna(data['freeFlow'], inplace=True)\n",
    "    return data\n",
    "\n",
    "# Create a 3D sequence of data\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x = sequence[i:end_ix, :6]\n",
    "        seq_y = sequence[end_ix, -4:]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def load_encoder_and_rmv_ovrlp(data, n_steps=N_STEPS):\n",
    "    data = rec_data_prp(data)\n",
    "    \n",
    "    # Load the saved encoder\n",
    "    label_encoder = joblib.load(ENCODER_LOAD_PATH)\n",
    "    data['min_osm_id'] = label_encoder.transform(data['min_osm_id'].astype(str))\n",
    "    \n",
    "    # Convert the 'timestamp' column to Unix time\n",
    "    data['timestamp'] = data['timestamp'].astype(np.int64) // 10**9\n",
    "    \n",
    "    data = data[['min_osm_id', 'timestamp', 'day', 'time', 'speed', 'freeFlow', 'target15', 'target30', 'target45', 'target60']]\n",
    "    data = data.values\n",
    "    a1, a2 = split_sequence(data, n_steps)\n",
    "    X, y = [], []\n",
    "    for i in range(len(a1)):\n",
    "        X.append(a1[i])\n",
    "        y.append(a2[i])\n",
    "    \n",
    "    # Convert X and y to float32 data type\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def speed_activation(x, freeFlow_speed):\n",
    "    freeFlow_speed = K.expand_dims(freeFlow_speed, axis=1)\n",
    "    return K.minimum(x, freeFlow_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fbab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model\n",
    "def model_config(X, y, batch_size, n_steps):\n",
    "    X = X[:, :, 2:6]\n",
    "    n_features = X.shape[2]\n",
    "    model = Sequential()\n",
    "    #model.add(TimeDistributed(Dense(5, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "    model.add(TimeDistributed(Dense(5, activation='relu'), input_shape=(n_steps, X.shape[-1])))\n",
    "    model.add(LSTM(5, activation='tanh', batch_input_shape=(batch_size, n_steps, n_features)))\n",
    "    model.add(Dense(4))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
    "    return model\n",
    "\n",
    "# Fit the model\n",
    "def model_fit(model, X, y, batch_size, epochs):\n",
    "    X = X[:, :, 2:6]\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    train_X = np.asarray(train_X).astype('float32')\n",
    "    test_X = np.asarray(test_X).astype('float32')\n",
    "    train_y = np.asarray(train_y).astype('float32')\n",
    "    test_y = np.asarray(test_y).astype('float32')\n",
    "    for epoch in range(epochs):\n",
    "        model.fit(train_X, train_y, epochs=1, batch_size=batch_size, validation_data=(test_X, test_y), shuffle=True)\n",
    "        model.reset_states()\n",
    "    return model\n",
    "\n",
    "# make prediction\n",
    "#print accuracy and show the prediction  \n",
    "def forecast_lstm(model, data): \n",
    "    X, y = rmv_ovrlp(data)  \n",
    "    X1 = X[:, :, 2:6]\n",
    "    yhat = model.predict(X1)\n",
    "    print('The prediction accuracy (MAPE) for next 15 minutes interval is: %.2f' % (MAPE(yhat[:,0], y[:,0])*100))\n",
    "    bins = np.arange(-5, 5.5, 0.25) #range and size of bins\n",
    "    # diff1 = yhat[:,0] - y[:,0]\n",
    "    # plt.hist(diff1, bins = bins)\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.title('Histogram of prediction errors for next 15 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    print('The prediction accuracy (MAPE) for next 30 minutes interval is: %.2f' % (MAPE(yhat[:,1], y[:,1])*100))\n",
    "    # diff2 = yhat[:,1] - y[:,1]\n",
    "    # plt.hist(diff2, bins = bins)\n",
    "    # plt.title('Histogram of prediction errors for next 30 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    print('The prediction accuracy (MAPE) for next 45 minutes interval is: %.2f' % (MAPE(yhat[:,2], y[:,2])*100))\n",
    "    # diff3 = yhat[:,2] - y[:,2]\n",
    "    # plt.hist(diff3, bins = bins)\n",
    "    # plt.title('Histogram of prediction errors for next 45 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    print('The prediction accuracy (MAPE) for next 60 minutes interval is: %.2f' % (MAPE(yhat[:,3], y[:,3])*100))\n",
    "    # diff4 = yhat[:,3] - y[:,3]\n",
    "    # plt.hist(diff4, bins = bins)\n",
    "    # plt.title('Histogram of prediction errors for next 60 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    output = np.concatenate((X[:, 0, :], yhat), axis=1)\n",
    "    output = pd.DataFrame(output, columns=['min_osm_id', 'timestamp', 'day', 'time', 'speed', 'freeFlow', 'Speed15', 'Speed30', 'Speed45', 'Speed60'])\n",
    "    output = output.round({'speed': 1, 'freeFlow': 1, 'Speed15': 1, 'Speed30': 1, 'Speed45': 1, 'Speed60': 1})\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_model(model, X, y):\n",
    "    X = X[:, :, 2:6]\n",
    "    y_pred = model.predict(X)\n",
    "    mape = MAPE(y, y_pred)\n",
    "    print(f\"Mean Absolute Percentage Error: {mape}\")\n",
    "    return mape\n",
    "\n",
    "# Run a repeated experiment\n",
    "def run(data_file_path, model_save_path, batch_size=BATCH_SIZE, epochs=EPOCHS, n_steps=N_STEPS):\n",
    "    # Load dataset\n",
    "    if not os.path.exists(data_file_path):\n",
    "        raise ValueError(f\"Dataset file not found at: {data_file_path}\")\n",
    "    df = pd.read_parquet(data_file_path).drop('Unnamed: 0', axis=1)\n",
    "    df = check_nan_values(df)\n",
    "\n",
    "    # Fit the base model\n",
    "    X, y = load_encoder_and_rmv_ovrlp(df)\n",
    "    model = model_config(X, y, batch_size, n_steps)\n",
    "    lstm_model = model_fit(model, X, y, batch_size, epochs)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(lstm_model, X, y)\n",
    "\n",
    "    # Save the model\n",
    "    if not os.path.exists(os.path.dirname(model_save_path)):\n",
    "        os.makedirs(os.path.dirname(model_save_path))\n",
    "    lstm_model.save(model_save_path)\n",
    "    return lstm_model\n",
    "\n",
    "# Entry point\n",
    "model = run(DATA_FILE_PATH, MODEL_SAVE_PATH, BATCH_SIZE, EPOCHS, N_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b14892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed, RepeatVector, Lambda\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Configurable parameters\n",
    "#DATA_FILE_PATH = '/Users/dharmik/Downloads/mac-RA/merged_data.csv'\n",
    "#MODEL_SAVE_PATH = '/Users/dharmik/Desktop/'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "N_STEPS = 12\n",
    "\n",
    "\n",
    "\n",
    "# Dataset columns\n",
    "REQUIRED_COLUMNS = ['min_osm_id', 'timestamp', 'speed', 'freeFlow']\n",
    "\n",
    "\n",
    "def convert_datetime(timestamp):\n",
    "    try:\n",
    "        return pd.to_datetime(timestamp, format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    except ValueError:\n",
    "        return pd.to_datetime(timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    \n",
    "# Preprocess dataset\n",
    "def rec_data_prp(data):\n",
    "    if not set(REQUIRED_COLUMNS).issubset(data.columns):\n",
    "        raise ValueError(f\"Dataset must contain the following columns: {REQUIRED_COLUMNS}\")\n",
    "    data = data.sort_values(by=['min_osm_id', 'timestamp'])\n",
    "    data['timestamp'] = data['timestamp'].apply(convert_datetime)\n",
    "    data['time'] = data['timestamp'].dt.strftime('%H')\n",
    "    data['day'] = data['timestamp'].dt.weekday\n",
    "    data['day'] = data['day'].apply(lambda x: '1' if x <= 4 else '0')  # weekday is 1 and weekend is 0\n",
    "    data['target15'] = data.speed.shift(-3)\n",
    "    data['target15'].fillna(data['freeFlow'], inplace=True)\n",
    "    data['target30'] = data.speed.shift(-6)\n",
    "    data['target30'].fillna(data['freeFlow'], inplace=True)\n",
    "    data['target45'] = data.speed.shift(-9)\n",
    "    data['target45'].fillna(data['freeFlow'], inplace=True)\n",
    "    data['target60'] = data.speed.shift(-12)\n",
    "    data['target60'].fillna(data['freeFlow'], inplace=True)\n",
    "    return data\n",
    "\n",
    "def check_nan_values(df):\n",
    "    # drop rows with NaN values in the dataframe\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Create a 3D sequence of data\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x = sequence[i:end_ix, :6]\n",
    "        seq_y = sequence[end_ix, -4:]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def load_encoder_and_rmv_ovrlp(data, n_steps=N_STEPS):\n",
    "    data = rec_data_prp(data)\n",
    "    \n",
    "    # Load the saved encoder\n",
    "    label_encoder = joblib.load(ENCODER_LOAD_PATH)\n",
    "    data['min_osm_id'] = label_encoder.transform(data['min_osm_id'].astype(str))\n",
    "    \n",
    "    # Convert the 'timestamp' column to Unix time\n",
    "    data['timestamp'] = data['timestamp'].astype(np.int64) // 10**9\n",
    "    \n",
    "    data = data[['min_osm_id', 'timestamp', 'day', 'time', 'speed', 'freeFlow', 'target15', 'target30', 'target45', 'target60']]\n",
    "    data = data.values\n",
    "    a1, a2 = split_sequence(data, n_steps)\n",
    "    X, y = [], []\n",
    "    for i in range(len(a1)):\n",
    "        X.append(a1[i])\n",
    "        y.append(a2[i])\n",
    "    \n",
    "    # Convert X and y to float32 data type\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def speed_activation(x, freeFlow_speed):\n",
    "    freeFlow_speed = K.expand_dims(freeFlow_speed, axis=1)\n",
    "    return K.minimum(x, freeFlow_speed)\n",
    "\n",
    "# # Configure the model\n",
    "# def model_config(X, y, batch_size, n_steps):\n",
    "#     X = X[:, :, 2:6]\n",
    "#     n_features = X.shape[2]\n",
    "#     model = Sequential()\n",
    "#     #model.add(TimeDistributed(Dense(5, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "#     model.add(TimeDistributed(Dense(5, activation='relu'), input_shape=(n_steps, X.shape[-1])))\n",
    "#     model.add(LSTM(5, activation='tanh', batch_input_shape=(batch_size, n_steps, n_features)))\n",
    "#     model.add(Dense(4))\n",
    "#     model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
    "#     return model\n",
    "\n",
    "# # Fit the model\n",
    "# def model_fit(model, X, y, batch_size, epochs):\n",
    "#     X = X[:, :, 2:6]\n",
    "#     train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "#     train_X = np.asarray(train_X).astype('float32')\n",
    "#     test_X = np.asarray(test_X).astype('float32')\n",
    "#     train_y = np.asarray(train_y).astype('float32')\n",
    "#     test_y = np.asarray(test_y).astype('float32')\n",
    "#     for epoch in range(epochs):\n",
    "#         model.fit(train_X, train_y, epochs=1, batch_size=batch_size, validation_data=(test_X, test_y), shuffle=True)\n",
    "#         model.reset_states()\n",
    "#     return model\n",
    "\n",
    "# Configure the model\n",
    "def model_config(X, y):\n",
    "    X = X[:, 0, 2:6]\n",
    "    n_features = X.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, activation='relu', batch_input_shape=(1, 1, n_features)))\n",
    "    model.add(LSTM(5, activation='tanh', stateful=True))\n",
    "    model.add(Dense(4))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
    "    return model\n",
    "\n",
    "# Fit the model\n",
    "def model_fit(model, X, y, epochs):\n",
    "    X = X[:, 0, 2:6]\n",
    "    n_features = X.shape[1]\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    train_X = np.asarray(train_X).astype('float32')\n",
    "    test_X = np.asarray(test_X).astype('float32')\n",
    "    train_y = np.asarray(train_y).astype('float32')\n",
    "    test_y = np.asarray(test_y).astype('float32')\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(train_X)):\n",
    "            model.train_on_batch(np.reshape(train_X[i], (1, 1, n_features)), np.reshape(train_y[i], (1, -1)))\n",
    "        model.reset_states()\n",
    "        for i in range(len(test_X)):\n",
    "            model.test_on_batch(np.reshape(test_X[i], (1, 1, n_features)), np.reshape(test_y[i], (1, -1)))\n",
    "    return model\n",
    "\n",
    "\n",
    "# make prediction\n",
    "#print accuracy and show the prediction  \n",
    "def forecast_lstm(model, data):\n",
    "    X, y = load_encoder_and_rmv_ovrlp(data)\n",
    "    X1 = X[:, :, 2:6] # select relevant features\n",
    "    n_features = X1.shape[2]\n",
    "    n_steps = X1.shape[1]\n",
    "    yhat = np.empty_like(y)\n",
    "    for i in range(len(X1)):\n",
    "        yhat[i] = model.predict(np.reshape(X1[i], (1, n_steps, n_features)))\n",
    "    print('The prediction accuracy (MAPE) for next 15 minutes interval is: %.2f' % (MAPE(yhat[:,0], y[:,0])*100))\n",
    "    bins = np.arange(-5, 5.5, 0.25) #range and size of bins\n",
    "    plt.hist(yhat[:,0] - y[:,0], bins, alpha=0.5, color='r', label='forecast') #forecast histogram\n",
    "    plt.hist(X[:,11,3] - y[:,0], bins, alpha=0.5, color='b', label='persistence') #persistence histogram\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # diff1 = yhat[:,0] - y[:,0]\n",
    "    # plt.hist(diff1, bins = bins)\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.title('Histogram of prediction errors for next 15 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    print('The prediction accuracy (MAPE) for next 30 minutes interval is: %.2f' % (MAPE(yhat[:,1], y[:,1])*100))\n",
    "    # diff2 = yhat[:,1] - y[:,1]\n",
    "    # plt.hist(diff2, bins = bins)\n",
    "    # plt.title('Histogram of prediction errors for next 30 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    print('The prediction accuracy (MAPE) for next 45 minutes interval is: %.2f' % (MAPE(yhat[:,2], y[:,2])*100))\n",
    "    # diff3 = yhat[:,2] - y[:,2]\n",
    "    # plt.hist(diff3, bins = bins)\n",
    "    # plt.title('Histogram of prediction errors for next 45 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    print('The prediction accuracy (MAPE) for next 60 minutes interval is: %.2f' % (MAPE(yhat[:,3], y[:,3])*100))\n",
    "    # diff4 = yhat[:,3] - y[:,3]\n",
    "    # plt.hist(diff4, bins = bins)\n",
    "    # plt.title('Histogram of prediction errors for next 60 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    output = np.concatenate((X[:, 0, :], yhat), axis=1)\n",
    "    output = pd.DataFrame(output, columns=['min_osm_id', 'timestamp', 'day', 'time', 'speed', 'freeFlow', 'Speed15', 'Speed30', 'Speed45', 'Speed60'])\n",
    "    output = output.round({'speed': 1, 'freeFlow': 1, 'Speed15': 1, 'Speed30': 1, 'Speed45': 1, 'Speed60': 1})\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Evaluate model performance\n",
    "# def evaluate_model(model, X, y):\n",
    "#     X = X[:, :, 2:6]\n",
    "#     y_pred = model.predict(X)\n",
    "#     mape = MAPE(y, y_pred)\n",
    "#     print(f\"Mean Absolute Percentage Error: {mape}\")\n",
    "#     return mape\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    X = X[:, :, 2:6]  # take only 2nd to 5th features\n",
    "    n_features = X.shape[2]  # number of features\n",
    "    n_steps = X.shape[1]  # number of time steps\n",
    "    y_pred = np.empty_like(y)\n",
    "    for i in range(len(X)):\n",
    "        X_sample = np.reshape(X[i], (1, n_steps, n_features))\n",
    "        y_pred[i] = model.predict(X_sample)\n",
    "    mape = MAPE(y, y_pred)\n",
    "    print(f\"Mean Absolute Percentage Error: {mape}\")\n",
    "    return mape\n",
    "\n",
    "    \n",
    "model = tf.keras.models.load_model('/Users/dharmik/Desktop/Model1/', compile=False)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
    "\n",
    "df = pd.read_parquet('/Users/dharmik/Downloads/compressed-results-2023-05-05 00_00_00.parquet').drop('Unnamed: 0', axis = 1)\n",
    "df = check_nan_values(df)\n",
    "\n",
    "\n",
    "# df = df.iloc[99000000:99700000,:] #selecting Tuesday October 27th 6:\n",
    "\n",
    "df = df.sort_values(by=['min_osm_id', 'timestamp'])\n",
    "output1 = forecast_lstm(model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a65ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cea4da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digitaltwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
