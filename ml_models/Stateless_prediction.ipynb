{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcef335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 11:30:42.239862: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed, RepeatVector, Lambda\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a7f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable parameters\n",
    "DATA_FILE_PATH = '/Users/dharmik/Downloads/compressed-results-2023-05-05 00_00_00.parquet'\n",
    "MODEL_SAVE_PATH = '/Users/dharmik/Desktop/Model1'\n",
    "ENCODER_LOAD_PATH = '/Users/dharmik/Downloads/mac-RA/encoder.joblib'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "N_STEPS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10149d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset columns\n",
    "REQUIRED_COLUMNS = ['min_osm_id', 'timestamp', 'speed', 'freeFlow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "741b1c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan_values(df):\n",
    "    # drop rows with NaN values in the dataframe\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df66a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime(timestamp):\n",
    "    try:\n",
    "        return pd.to_datetime(timestamp, format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    except ValueError:\n",
    "        return pd.to_datetime(timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def rec_data_prp(data):\n",
    "    if not set(REQUIRED_COLUMNS).issubset(data.columns):\n",
    "        raise ValueError(f\"Dataset must contain the following columns: {REQUIRED_COLUMNS}\")\n",
    "    data = data.sort_values(by=['min_osm_id', 'timestamp'])\n",
    "    data['timestamp'] = data['timestamp'].apply(convert_datetime)\n",
    "    data['time'] = data['timestamp'].dt.strftime('%H')\n",
    "    data['day'] = data['timestamp'].dt.weekday\n",
    "    data['day'] = data['day'].apply(lambda x: '1' if x <= 4 else '0')  # weekday is 1 and weekend is 0\n",
    "    data['target15'] = data.speed.shift(-3)\n",
    "    data['target15'].fillna(data['freeFlow'], inplace=True)\n",
    "    data['target30'] = data.speed.shift(-6)\n",
    "    data['target30'].fillna(data['freeFlow'], inplace=True)\n",
    "    data['target45'] = data.speed.shift(-9)\n",
    "    data['target45'].fillna(data['freeFlow'], inplace=True)\n",
    "    data['target60'] = data.speed.shift(-12)\n",
    "    data['target60'].fillna(data['freeFlow'], inplace=True)\n",
    "    return data\n",
    "\n",
    "# Create a 3D sequence of data\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x = sequence[i:end_ix, :6]\n",
    "        seq_y = sequence[end_ix, -4:]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def load_encoder_and_rmv_ovrlp(data, n_steps=N_STEPS):\n",
    "    data = rec_data_prp(data)\n",
    "    \n",
    "    # Load the saved encoder\n",
    "    label_encoder = joblib.load(ENCODER_LOAD_PATH)\n",
    "    data['min_osm_id'] = label_encoder.transform(data['min_osm_id'].astype(str))\n",
    "    \n",
    "    # Convert the 'timestamp' column to Unix time\n",
    "    data['timestamp'] = data['timestamp'].astype(np.int64) // 10**9\n",
    "    \n",
    "    data = data[['min_osm_id', 'timestamp', 'day', 'time', 'speed', 'freeFlow', 'target15', 'target30', 'target45', 'target60']]\n",
    "    data = data.values\n",
    "    a1, a2 = split_sequence(data, n_steps)\n",
    "    X, y = [], []\n",
    "    for i in range(len(a1)):\n",
    "        X.append(a1[i])\n",
    "        y.append(a2[i])\n",
    "    \n",
    "    # Convert X and y to float32 data type\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def speed_activation(x, freeFlow_speed):\n",
    "    freeFlow_speed = K.expand_dims(freeFlow_speed, axis=1)\n",
    "    return K.minimum(x, freeFlow_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20fbab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163848/163848 [==============================] - 723s 4ms/step - loss: 4.5541 - mean_absolute_percentage_error: 7.5949 - val_loss: 1.9730 - val_mean_absolute_percentage_error: 6.2657\n",
      "204810/204810 [==============================] - 326s 2ms/step\n",
      "Mean Absolute Percentage Error: 0.06301330029964447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/dharmik/Desktop/Model1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/dharmik/Desktop/Model1/assets\n"
     ]
    }
   ],
   "source": [
    "# Configure the model\n",
    "def model_config(X, y, batch_size, n_steps):\n",
    "    X = X[:, :, 2:6]\n",
    "    n_features = X.shape[2]\n",
    "    model = Sequential()\n",
    "    #model.add(TimeDistributed(Dense(5, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "    model.add(TimeDistributed(Dense(5, activation='relu'), input_shape=(n_steps, X.shape[-1])))\n",
    "    model.add(LSTM(5, activation='tanh', batch_input_shape=(batch_size, n_steps, n_features)))\n",
    "    model.add(Dense(4))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
    "    return model\n",
    "\n",
    "# Fit the model\n",
    "def model_fit(model, X, y, batch_size, epochs):\n",
    "    X = X[:, :, 2:6]\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    train_X = np.asarray(train_X).astype('float32')\n",
    "    test_X = np.asarray(test_X).astype('float32')\n",
    "    train_y = np.asarray(train_y).astype('float32')\n",
    "    test_y = np.asarray(test_y).astype('float32')\n",
    "    for epoch in range(epochs):\n",
    "        model.fit(train_X, train_y, epochs=1, batch_size=batch_size, validation_data=(test_X, test_y), shuffle=True)\n",
    "        model.reset_states()\n",
    "    return model\n",
    "\n",
    "# make prediction\n",
    "#print accuracy and show the prediction  \n",
    "def forecast_lstm(model, data): \n",
    "    X, y = rmv_ovrlp(data)  \n",
    "    X1 = X[:, :, 2:6]\n",
    "    yhat = model.predict(X1)\n",
    "    print('The prediction accuracy (MAPE) for next 15 minutes interval is: %.2f' % (MAPE(yhat[:,0], y[:,0])*100))\n",
    "    bins = np.arange(-5, 5.5, 0.25) #range and size of bins\n",
    "    # diff1 = yhat[:,0] - y[:,0]\n",
    "    # plt.hist(diff1, bins = bins)\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.title('Histogram of prediction errors for next 15 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    print('The prediction accuracy (MAPE) for next 30 minutes interval is: %.2f' % (MAPE(yhat[:,1], y[:,1])*100))\n",
    "    # diff2 = yhat[:,1] - y[:,1]\n",
    "    # plt.hist(diff2, bins = bins)\n",
    "    # plt.title('Histogram of prediction errors for next 30 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    print('The prediction accuracy (MAPE) for next 45 minutes interval is: %.2f' % (MAPE(yhat[:,2], y[:,2])*100))\n",
    "    # diff3 = yhat[:,2] - y[:,2]\n",
    "    # plt.hist(diff3, bins = bins)\n",
    "    # plt.title('Histogram of prediction errors for next 45 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    print('The prediction accuracy (MAPE) for next 60 minutes interval is: %.2f' % (MAPE(yhat[:,3], y[:,3])*100))\n",
    "    # diff4 = yhat[:,3] - y[:,3]\n",
    "    # plt.hist(diff4, bins = bins)\n",
    "    # plt.title('Histogram of prediction errors for next 60 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    output = np.concatenate((X[:, 0, :], yhat), axis=1)\n",
    "    output = pd.DataFrame(output, columns=['min_osm_id', 'timestamp', 'day', 'time', 'speed', 'freeFlow', 'Speed15', 'Speed30', 'Speed45', 'Speed60'])\n",
    "    output = output.round({'speed': 1, 'freeFlow': 1, 'Speed15': 1, 'Speed30': 1, 'Speed45': 1, 'Speed60': 1})\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_model(model, X, y):\n",
    "    X = X[:, :, 2:6]\n",
    "    y_pred = model.predict(X)\n",
    "    mape = MAPE(y, y_pred)\n",
    "    print(f\"Mean Absolute Percentage Error: {mape}\")\n",
    "    return mape\n",
    "\n",
    "# Run a repeated experiment\n",
    "def run(data_file_path, model_save_path, batch_size=BATCH_SIZE, epochs=EPOCHS, n_steps=N_STEPS):\n",
    "    # Load dataset\n",
    "    if not os.path.exists(data_file_path):\n",
    "        raise ValueError(f\"Dataset file not found at: {data_file_path}\")\n",
    "    df = pd.read_parquet(data_file_path).drop('Unnamed: 0', axis=1)\n",
    "    df = check_nan_values(df)\n",
    "\n",
    "    # Fit the base model\n",
    "    X, y = load_encoder_and_rmv_ovrlp(df)\n",
    "    model = model_config(X, y, batch_size, n_steps)\n",
    "    lstm_model = model_fit(model, X, y, batch_size, epochs)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(lstm_model, X, y)\n",
    "\n",
    "    # Save the model\n",
    "    if not os.path.exists(os.path.dirname(model_save_path)):\n",
    "        os.makedirs(os.path.dirname(model_save_path))\n",
    "    lstm_model.save(model_save_path)\n",
    "    return lstm_model\n",
    "\n",
    "# Entry point\n",
    "model = run(DATA_FILE_PATH, MODEL_SAVE_PATH, BATCH_SIZE, EPOCHS, N_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b14892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 15:08:35.370629: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 243>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# df = df.iloc[99000000:99700000,:] #selecting Tuesday October 27th 6:\u001b[39;00m\n\u001b[1;32m    242\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_osm_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 243\u001b[0m output1 \u001b[38;5;241m=\u001b[39m \u001b[43mforecast_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mforecast_lstm\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforecast_lstm\u001b[39m(model, data):\n\u001b[0;32m--> 156\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mload_encoder_and_rmv_ovrlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     X1 \u001b[38;5;241m=\u001b[39m X[:, :, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m6\u001b[39m] \u001b[38;5;66;03m# select relevant features\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m X1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mload_encoder_and_rmv_ovrlp\u001b[0;34m(data, n_steps)\u001b[0m\n\u001b[1;32m     72\u001b[0m data \u001b[38;5;241m=\u001b[39m rec_data_prp(data)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Load the saved encoder\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241m.\u001b[39mload(ENCODER_LOAD_PATH)\n\u001b[1;32m     76\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_osm_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mtransform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_osm_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Convert the 'timestamp' column to Unix time\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed, RepeatVector, Lambda\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Configurable parameters\n",
    "#DATA_FILE_PATH = '/Users/dharmik/Downloads/mac-RA/merged_data.csv'\n",
    "#MODEL_SAVE_PATH = '/Users/dharmik/Desktop/'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "N_STEPS = 12\n",
    "\n",
    "\n",
    "\n",
    "# Dataset columns\n",
    "REQUIRED_COLUMNS = ['min_osm_id', 'timestamp', 'speed', 'freeFlow']\n",
    "\n",
    "\n",
    "def convert_datetime(timestamp):\n",
    "    try:\n",
    "        return pd.to_datetime(timestamp, format='%Y-%m-%d %H:%M:%S.%f')\n",
    "    except ValueError:\n",
    "        return pd.to_datetime(timestamp, format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    \n",
    "# Preprocess dataset\n",
    "def rec_data_prp(data):\n",
    "    if not set(REQUIRED_COLUMNS).issubset(data.columns):\n",
    "        raise ValueError(f\"Dataset must contain the following columns: {REQUIRED_COLUMNS}\")\n",
    "    data = data.sort_values(by=['min_osm_id', 'timestamp'])\n",
    "    data['timestamp'] = data['timestamp'].apply(convert_datetime)\n",
    "    data['time'] = data['timestamp'].dt.strftime('%H')\n",
    "    data['day'] = data['timestamp'].dt.weekday\n",
    "    data['day'] = data['day'].apply(lambda x: '1' if x <= 4 else '0')  # weekday is 1 and weekend is 0\n",
    "    data['target15'] = data.speed.shift(-3)\n",
    "    data['target15'].fillna(data['freeFlow'], inplace=True)\n",
    "    data['target30'] = data.speed.shift(-6)\n",
    "    data['target30'].fillna(data['freeFlow'], inplace=True)\n",
    "    data['target45'] = data.speed.shift(-9)\n",
    "    data['target45'].fillna(data['freeFlow'], inplace=True)\n",
    "    data['target60'] = data.speed.shift(-12)\n",
    "    data['target60'].fillna(data['freeFlow'], inplace=True)\n",
    "    return data\n",
    "\n",
    "def check_nan_values(df):\n",
    "    # drop rows with NaN values in the dataframe\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Create a 3D sequence of data\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x = sequence[i:end_ix, :6]\n",
    "        seq_y = sequence[end_ix, -4:]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def load_encoder_and_rmv_ovrlp(data, n_steps=N_STEPS):\n",
    "    data = rec_data_prp(data)\n",
    "    \n",
    "    # Load the saved encoder\n",
    "    label_encoder = joblib.load(ENCODER_LOAD_PATH)\n",
    "    data['min_osm_id'] = label_encoder.transform(data['min_osm_id'].astype(str))\n",
    "    \n",
    "    # Convert the 'timestamp' column to Unix time\n",
    "    data['timestamp'] = data['timestamp'].astype(np.int64) // 10**9\n",
    "    \n",
    "    data = data[['min_osm_id', 'timestamp', 'day', 'time', 'speed', 'freeFlow', 'target15', 'target30', 'target45', 'target60']]\n",
    "    data = data.values\n",
    "    a1, a2 = split_sequence(data, n_steps)\n",
    "    X, y = [], []\n",
    "    for i in range(len(a1)):\n",
    "        X.append(a1[i])\n",
    "        y.append(a2[i])\n",
    "    \n",
    "    # Convert X and y to float32 data type\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def speed_activation(x, freeFlow_speed):\n",
    "    freeFlow_speed = K.expand_dims(freeFlow_speed, axis=1)\n",
    "    return K.minimum(x, freeFlow_speed)\n",
    "\n",
    "# # Configure the model\n",
    "# def model_config(X, y, batch_size, n_steps):\n",
    "#     X = X[:, :, 2:6]\n",
    "#     n_features = X.shape[2]\n",
    "#     model = Sequential()\n",
    "#     #model.add(TimeDistributed(Dense(5, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "#     model.add(TimeDistributed(Dense(5, activation='relu'), input_shape=(n_steps, X.shape[-1])))\n",
    "#     model.add(LSTM(5, activation='tanh', batch_input_shape=(batch_size, n_steps, n_features)))\n",
    "#     model.add(Dense(4))\n",
    "#     model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
    "#     return model\n",
    "\n",
    "# # Fit the model\n",
    "# def model_fit(model, X, y, batch_size, epochs):\n",
    "#     X = X[:, :, 2:6]\n",
    "#     train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "#     train_X = np.asarray(train_X).astype('float32')\n",
    "#     test_X = np.asarray(test_X).astype('float32')\n",
    "#     train_y = np.asarray(train_y).astype('float32')\n",
    "#     test_y = np.asarray(test_y).astype('float32')\n",
    "#     for epoch in range(epochs):\n",
    "#         model.fit(train_X, train_y, epochs=1, batch_size=batch_size, validation_data=(test_X, test_y), shuffle=True)\n",
    "#         model.reset_states()\n",
    "#     return model\n",
    "\n",
    "# Configure the model\n",
    "def model_config(X, y):\n",
    "    X = X[:, 0, 2:6]\n",
    "    n_features = X.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, activation='relu', batch_input_shape=(1, 1, n_features)))\n",
    "    model.add(LSTM(5, activation='tanh', stateful=True))\n",
    "    model.add(Dense(4))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
    "    return model\n",
    "\n",
    "# Fit the model\n",
    "def model_fit(model, X, y, epochs):\n",
    "    X = X[:, 0, 2:6]\n",
    "    n_features = X.shape[1]\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    train_X = np.asarray(train_X).astype('float32')\n",
    "    test_X = np.asarray(test_X).astype('float32')\n",
    "    train_y = np.asarray(train_y).astype('float32')\n",
    "    test_y = np.asarray(test_y).astype('float32')\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(train_X)):\n",
    "            model.train_on_batch(np.reshape(train_X[i], (1, 1, n_features)), np.reshape(train_y[i], (1, -1)))\n",
    "        model.reset_states()\n",
    "        for i in range(len(test_X)):\n",
    "            model.test_on_batch(np.reshape(test_X[i], (1, 1, n_features)), np.reshape(test_y[i], (1, -1)))\n",
    "    return model\n",
    "\n",
    "\n",
    "# make prediction\n",
    "#print accuracy and show the prediction  \n",
    "def forecast_lstm(model, data):\n",
    "    X, y = load_encoder_and_rmv_ovrlp(data)\n",
    "    X1 = X[:, :, 2:6] # select relevant features\n",
    "    n_features = X1.shape[2]\n",
    "    n_steps = X1.shape[1]\n",
    "    yhat = np.empty_like(y)\n",
    "    for i in range(len(X1)):\n",
    "        yhat[i] = model.predict(np.reshape(X1[i], (1, n_steps, n_features)))\n",
    "    print('The prediction accuracy (MAPE) for next 15 minutes interval is: %.2f' % (MAPE(yhat[:,0], y[:,0])*100))\n",
    "    bins = np.arange(-5, 5.5, 0.25) #range and size of bins\n",
    "    plt.hist(yhat[:,0] - y[:,0], bins, alpha=0.5, color='r', label='forecast') #forecast histogram\n",
    "    plt.hist(X[:,11,3] - y[:,0], bins, alpha=0.5, color='b', label='persistence') #persistence histogram\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # diff1 = yhat[:,0] - y[:,0]\n",
    "    # plt.hist(diff1, bins = bins)\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.title('Histogram of prediction errors for next 15 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    print('The prediction accuracy (MAPE) for next 30 minutes interval is: %.2f' % (MAPE(yhat[:,1], y[:,1])*100))\n",
    "    # diff2 = yhat[:,1] - y[:,1]\n",
    "    # plt.hist(diff2, bins = bins)\n",
    "    # plt.title('Histogram of prediction errors for next 30 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    print('The prediction accuracy (MAPE) for next 45 minutes interval is: %.2f' % (MAPE(yhat[:,2], y[:,2])*100))\n",
    "    # diff3 = yhat[:,2] - y[:,2]\n",
    "    # plt.hist(diff3, bins = bins)\n",
    "    # plt.title('Histogram of prediction errors for next 45 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    print('The prediction accuracy (MAPE) for next 60 minutes interval is: %.2f' % (MAPE(yhat[:,3], y[:,3])*100))\n",
    "    # diff4 = yhat[:,3] - y[:,3]\n",
    "    # plt.hist(diff4, bins = bins)\n",
    "    # plt.title('Histogram of prediction errors for next 60 minutes')\n",
    "    # plt.xlabel('Relative percentage error')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.gcf().set_dpi(300)\n",
    "    # plt.show()\n",
    "    #..........\n",
    "    output = np.concatenate((X[:, 0, :], yhat), axis=1)\n",
    "    output = pd.DataFrame(output, columns=['min_osm_id', 'timestamp', 'day', 'time', 'speed', 'freeFlow', 'Speed15', 'Speed30', 'Speed45', 'Speed60'])\n",
    "    output = output.round({'speed': 1, 'freeFlow': 1, 'Speed15': 1, 'Speed30': 1, 'Speed45': 1, 'Speed60': 1})\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Evaluate model performance\n",
    "# def evaluate_model(model, X, y):\n",
    "#     X = X[:, :, 2:6]\n",
    "#     y_pred = model.predict(X)\n",
    "#     mape = MAPE(y, y_pred)\n",
    "#     print(f\"Mean Absolute Percentage Error: {mape}\")\n",
    "#     return mape\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    X = X[:, :, 2:6]  # take only 2nd to 5th features\n",
    "    n_features = X.shape[2]  # number of features\n",
    "    n_steps = X.shape[1]  # number of time steps\n",
    "    y_pred = np.empty_like(y)\n",
    "    for i in range(len(X)):\n",
    "        X_sample = np.reshape(X[i], (1, n_steps, n_features))\n",
    "        y_pred[i] = model.predict(X_sample)\n",
    "    mape = MAPE(y, y_pred)\n",
    "    print(f\"Mean Absolute Percentage Error: {mape}\")\n",
    "    return mape\n",
    "\n",
    "    \n",
    "model = tf.keras.models.load_model('/Users/dharmik/Desktop/Model1/', compile=False)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
    "\n",
    "df = pd.read_parquet('/Users/dharmik/Downloads/compressed-results-2023-05-05 00_00_00.parquet').drop('Unnamed: 0', axis = 1)\n",
    "df = check_nan_values(df)\n",
    "\n",
    "\n",
    "# df = df.iloc[99000000:99700000,:] #selecting Tuesday October 27th 6:\n",
    "\n",
    "df = df.sort_values(by=['min_osm_id', 'timestamp'])\n",
    "output1 = forecast_lstm(model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a65ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cea4da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
